#+TITLE:     Charity Navigator Web Data Scraper
#+AUTHOR:    Jeffrey Horn
#+EMAIL:     jrhorn424@gmail.com
#+DATE:      2011-04-09 Sat
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:

#+BABEL: :tangle yes

* Resources
:PROPERTIES:
:ID: 0622F934-94E2-41C0-8605-0FA670CF16A0
:END:
- [[file:search-api.pdf][Charity Navigator Search API (local only)]]
- [[http://lxml.de][lxml]]
  - [[http://paste.pound-python.org/show/5082/][Scraper-Specific Mapping Code (courtesy bob2 in #python)]]
  - [[http://infohost.nmt.edu/tcc/help/pubs/pylxml/pylxml.pdf][Handy lxml overview]]
- [[http://www.w3.org/TR/xpath/][Xpath Doc]]
* Tasks
** TODO Add other data from charity [[http://www.charitynavigator.org/index.cfm?bay=search.summary&orgid=12123][summary pages]]
** TODO Look into cookie handling to log in and collect historical data
** TODO Add arguments to script to turn on verbosity (print lines in each loop) and persistence (saving the XML/HTML docs to disk)
** TODO Work on error handling
** TODO Figure out a better way to do CSV header row
* Script
** Setup
We'll use [[http://www.lxml.de][lxml]] to navigate both the XML and HTML documents and [[http://personalpages.tds.net/~kent37/kk/00010.html][urllib2]]
to fetch them. The [[http://docs.python.org/library/csv.html][csv]] module is used to store the generated Python
dictionary in human- and machine-readable form.
#+begin_src python
  import urllib2
  import lxml.etree
  import lxml.html
  import csv
#+end_src

The Charity Navigator API application ID and initial record number
will be stored in variables. This gives me an excuse to use string
formatting (I'm easily amused) and makes the code prettier (I'm easily
distracted).
#+begin_src python
  appid = input('Please enter your API key or app ID:')
  rec = 1
#+end_src
** Determine Maximum Records
We use lxml to find the total number of records for our query. This is
automated since the number of records might change at any moment, but
it is also very fragile! If Charity Navigator changes the way it
returns results, this will break. 
#+begin_src python
  data = urllib2.urlopen("http://www.charitynavigator.org/feeds/search4/?appid=%d&fromrec=%d" % (appid, rec))
  doc = lxml.etree.parse(data)
  data.close()

  maxrec = int(doc.getroot().get('total'))
#+end_src

The number of the last record is stored conveniently in the "total"
attribute of the resultset tag. getroot() puts us at the top of the
document, into the resultset tag. get() returns the text associated
with that attribute. We have to convert that string into an integer to
use later.
** Combine Streams into Single Dataset
First, download a page into memory, then parse it using lxml and
Xpath.[fn:: bob2 in #python@irc.freenode.net helped with much of
this.] Use lxml to list each charity record, and iterchildren() and
some magic to create a dictionary from the XML. Non-ascii codes were
being returned for some element text, and we have to encode everything
as UTF-8 to make the csv module happy later.
#+begin_src python
  results = []
  
  for rec in range(1, maxrec, 25):
      # print "Downloading dataset %d out of %d" % ((1+rec/25),maxrec/25)
      data = urllib2.urlopen("http://www.charitynavigator.org/feeds/search4/?appid=%d&fromrec=%d" % (appid, rec)) 
      doc = lxml.etree.parse(data)
      for charity in doc.findall('charity'):
          try:
              results.append(dict((item.tag, item.text.encode('utf-8')) for item in charity.iterchildren()))
          except:
              # print "Exception (probably utf-8 encoder)"
              results.append(dict((item.tag, item.text) for item in charity.iterchildren()))
      # with open('xml/%05d.xml' % rec,'wb') as f:
      #       f.write(data.read()) # save a copy, just in case
      data.close()
#+end_src
** Download Each Charity's Fundraising Efficiency
The XML files in the previous section gave us a lot of data, but there
is much more data available on each charity's page on the Charity
Navigator website. This part of the code scrapes that data, and
doesn't technically require API access to do. Again, we have to be
careful about character encoding.

The way we scrape each bit of data is to first search for the data key
text on the webpage (for example "Fundraising Efficiency"). Then, the
next cell in the table usually holds that bit of data, so we walk up
the tree to the parent (the cell holding the data key) and over to the
next sibling (the cell holding the data or value). We grab the data
and add it to the dictionary.
#+begin_src python
  for charity in results:
      # print "Downloading efficiency for organization " + charity['orgid'] + " (%d out of %d)" % (charity+1,len(results))
      doc = lxml.html.parse(urllib2.urlopen(charity['url'],timeout=600)) # why does this have to be compressed to one line?
      for element in doc.xpath('//div/table/tr/td/a'):
          if element.text=='Fundraising Efficiency':
              try:
                  rating = element.getparent().getnext().text.encode('utf-8')
              except:
                  # print "Exception (probably utf-8 encoder)"
                  rating = element.getparent().getnext().text
      charity['efficiency'] = rating
      # with open('html/$05d.html' % charity['orgid'], 'wb') as f:
      #     f.write(data.read())
#+end_src
** Export CSV File
We want to export using CSV for sharing and using in statistical
software.
#+begin_src python
  with open('output.csv','wb') as f:
      fn = "orgid charity_name category efficiency".split() # this will be very annoying to maintain
      writer=csv.DictWriter(f, fieldnames=fn, extrasaction='ignore')
      headers={}
      for n in fn:
          headers[n]=n
      writer.writerow(headers)
      for charity in results:
          writer.writerow(charity)
#+end_src
