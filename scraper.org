#+TITLE:     Charity Navigator Web Data Scraper
#+AUTHOR:    Jeffrey Horn
#+EMAIL:     jrhorn424@gmail.com
#+DATE:      2011-04-09 Sat
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:

#+BABEL: :tangle yes

* Resources
:PROPERTIES:
:ID: 0622F934-94E2-41C0-8605-0FA670CF16A0
:END:
- [[http://doc.scrapy.org/][Scrapy]]
- [[file:search-api.pdf][Charity Navigator Search API]]
- [[http://www.w3.org/TR/xpath/][Xpath Doc]]
- [[http://lxml.de][lxml]]
  - [[http://lxml.de/FAQ.html#how-can-i-map-an-xml-tree-into-a-dict-of-dicts][Mapping XML into Dicts (not used)]]
  - [[http://paste.pound-python.org/show/5082/][Scraper-Specific Mapping Code (courtesy bob2 in #python)]]
  - [[http://infohost.nmt.edu/tcc/help/pubs/pylxml/pylxml.pdf][Handy lxml overview]]
- [[http://docs.tablib.org/en/latest/index.html][tablib]]
* Overview
- [2/2] Download first results page for all charities
  - [X] Store maximum number of records in a variable
  - [X] Determine how many queries to run based on this number (results
    returned 25 at a time)
- [X] Download results in sets of 25, writing each XML stream to disk
- [X] Combine streams into single dataset with one record per row. XML
  returns dictionary style, so keys can be column headers
- [X] Download each charity's ratings page using the unique org id already
  stored in the dataset
- [X] Search and parse these pages for the financial rating score of
  interest and store as a new column
* Script
** Setup
We'll use [[http://www.crummy.com/software/BeautifulSoup/documentation.html][BeautifulSoup]] to navigate the document and [[http://personalpages.tds.net/~kent37/kk/00010.html][urllib2]] to fetch
the XML feed pages.
#+begin_src python
  import urllib2
  import lxml.etree
  import lxml.html
  import csv
#+end_src

The Charity Navigator API application ID and initial record number
will be stored in variables. This gives me an excuse to use string
formatting (I'm easily amused) and makes the code prettier (I'm easily
distracted).
#+begin_src python
  appid = 99487923783
  rec = 1
#+end_src
** Determine Maximum Records
This needs to be automated. Try downloading the XML feed and using
Xpath/BeautifulSoup to assign the maximum records to a variable to use
later.
#+begin_src python
  data = urllib2.urlopen("http://www.charitynavigator.org/feeds/search4/?appid=%d&fromrec=%d" % (appid, rec))
  doc = lxml.etree.parse(data)
  data.close()
#+end_src

Closing the page is best practice. Since BS returns unicode
characters, we have to wrap everything in int() to convert the string
to an integer. The path selectors look into the "resultset" tag (the
root of Charity Nav query results), and this code is fragile. Check
here for errors if CN changes the way they return results. The number
of the last record is stored conveniently in the "total" attribute of
the resultset tag.
#+begin_src python
  maxrec = int(doc.getroot().get('total'))
#+end_src

We want to download each result set up to the one containing the
maximum number. Since we have to download each page and load it into
memory to create the dictionary, the download "persistence" code is
moved into the next section.
*************** Alternate Iteration I                :noexport:
So that number divided by 25 will give us the number of sets to
iterate through.
*************** END
*************** Alternate Iteration II               :noexport:
Fun diversion, not sure if it is useful: maxrec modulo 25 will give us
the remainder of items in the last set, so the last set downloaded
will start at maxrec - (maxrec modulo 25) + 1.
*************** END
** Combine Streams into Single Dataset
First, download a page into memory, then parse it using lxml and
Xpath. Many thanks to bob2 in #python. Download each result set, and
parse using lxml. Use lxml to list each charity record, and
iterchildren() and some magic to create a dictionary from the XML.
#+begin_src python
  results = []
  
  for rec in range(1, maxrec, 25):
      print "Downloading dataset %d out of %d" % ((1+rec/25),maxrec/25)
      data = urllib2.urlopen("http://www.charitynavigator.org/feeds/search4/?appid=%d&fromrec=%d" % (appid, rec)) 
      doc = lxml.etree.parse(data)
      for charity in doc.findall('charity'):
          results.append(dict((item.tag, item.text.encode('utf-8')) for item in charity.iterchildren()))
      # with open('xml/%05d.xml' % rec,'wb') as f:
      #       f.write(data.read()) # save a copy, just in case
      data.close()
#+end_src
** Download Each Charity's Fundraising Efficiency
#+begin_src python
  for charity in results:
      print "Downloading efficiency for organization " + charity['orgid']
      doc = lxml.html.parse(urllib2.urlopen(charity['url'],timeout=600))
      for element in doc.xpath('//div/table/tr/td/a'):
          if element.text=='Fundraising Efficiency':
              rating = element.getparent().getnext().text.encode('utf-8')
      charity['efficiency'] = rating
      # with open('html/$05d.html' % charity['orgid'], 'wb') as f:
      #     f.write(data.read())
      data.close()
#+end_src
** Export CSV File
We want to export using CSV for sharing and using in statistical
software. csv.DictWriter class 
#+begin_src python
  with open('output.csv','wb') as f:
      fn = "orgid charity_name category efficiency".split()
      writer=csv.DictWriter(f, fieldnames=fn, extrasaction='ignore')
      headers={}
      for n in fn:
          headers[n]=n
      writer.writerow(headers)
      for charity in results:
          writer.writerow(charity)
#+end_src


