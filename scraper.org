#+TITLE:     Charity Navigator Web Data Scraper
#+AUTHOR:    Jeffrey Horn
#+EMAIL:     jrhorn424@gmail.com
#+DATE:      2011-04-09 Sat
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:

#+BABEL: :tangle yes

* Resources
:PROPERTIES:
:ID: 0622F934-94E2-41C0-8605-0FA670CF16A0
:END:
- [[file:search-api.pdf][Charity Navigator Search API (local only)]]
- [[http://lxml.de][lxml]]
  - [[http://paste.pound-python.org/show/5082/][Scraper-Specific Mapping Code (courtesy bob2 in #python)]]
  - [[http://infohost.nmt.edu/tcc/help/pubs/pylxml/pylxml.pdf][Handy lxml overview]]
- [[http://www.w3.org/TR/xpath/][Xpath Doc]]
* Tasks
** TODO Add other data from charity [[http://www.charitynavigator.org/index.cfm?bay=search.summary&orgid=12123][summary pages]]
** TODO Look into cookie handling to log in and collect historical data
** TODO Add arguments to script 
- To turn on verbosity (print lines in each loop)
- For persistence (saving the XML/HTML docs to disk)
  - So I do not constantly hit the XML over and over when I encounter
    a bug in the scraper
  - Could write one row to the CSV file at a time
** TODO Work on error handling
** TODO Figure out a better way to do CSV header row
** TODO Replace some regexp's with s.replace('this','that') method
** TODO Work on exporting all new data to CSV
** TODO Add command line arguments to save searches
* Script
** Setup
We'll use [[http://www.lxml.de][lxml]] to navigate both the XML and HTML documents and [[http://personalpages.tds.net/~kent37/kk/00010.html][urllib2]]
to fetch them. The [[http://docs.python.org/library/csv.html][csv]] module is used to store the generated Python
dictionary in human- and machine-readable form.
#+begin_src python
  import urllib2
  import lxml.etree
  import lxml.html
  import csv
  import re
#+end_src

The Charity Navigator API application ID and initial record number
will be stored in variables. This gives me an excuse to use string
formatting (I'm easily amused) and makes the code prettier (I'm easily
distracted).
#+begin_src python
  appid = input('Please enter your API key or app ID:')
  rec = 1
#+end_src
** Determine Maximum Records
We use lxml to find the total number of records for our query. This is
automated since the number of records might change at any moment, but
it is also very fragile! If Charity Navigator changes the way it
returns results, this will break. 
#+begin_src python
  doc = lxml.etree.parse(urllib2.urlopen("http://www.charitynavigator.org/feeds/search4/?appid=%d&fromrec=%d" % (appid, rec)))
   
  maxrec = int(doc.getroot().get('total'))
#+end_src

The number of the last record is stored conveniently in the "total"
attribute of the resultset tag. getroot() puts us at the top of the
document, into the resultset tag. get() returns the text associated
with that attribute. We have to convert that string into an integer to
use later.
** Combine Streams into Single Dataset
First, download a page into memory, then parse it using lxml and
Xpath.[fn:: bob2 in #python@irc.freenode.net helped with much of
this.] Use lxml to list each charity record, and iterchildren() and
some magic to create a dictionary from the XML. Non-ascii codes were
being returned for some element text, and we have to encode everything
as UTF-8 to make the csv module happy later.
#+begin_src python
  results = []
  
  for rec in range(1, maxrec, 25):
      # print "Downloading dataset %d out of %d" % ((1+rec/25),maxrec/25)
      doc = lxml.etree.parse(urllib2.urlopen("http://www.charitynavigator.org/feeds/search4/?appid=%d&fromrec=%d" % (appid, rec)))
      for charity in doc.findall('charity'):
          try:
              results.append(dict((item.tag, item.text.encode('utf-8')) for item in charity.iterchildren()))
          except:
              # print "Exception (probably utf-8 encoder)"
              results.append(dict((item.tag, item.text) for item in charity.iterchildren()))
      # with open('xml/%05d.xml' % rec,'wb') as f:
      #       f.write(data.read()) # save a copy, just in case
#+end_src
** Download Individual Charity Data
The XML files in the previous section gave us a lot of data, but there
is much more data available on each charity's page on the Charity
Navigator website. This part of the code scrapes that data, and
doesn't technically require API access to do. Again, we have to be
careful about character encoding.

The way we scrape each bit of data is to first search for the data key
text on the webpage (for example "Fundraising Efficiency"). Then, the
next cell in the table usually holds that bit of data, so we walk up
the tree to the parent (the cell holding the data key) and over to the
next sibling (the cell holding the data or value). We grab the data
and add it to the dictionary.
#+begin_src python
  for i, charity in enumerate(results):
      doc = lxml.html.parse(urllib2.urlopen(charity['url'],timeout=600))
      print "Processing charity %s out of %s, id: %s" % (i, len(results), charity['orgid'])
      def rating(path):
          """ Take xpath to tabular data and clean it up by removing paretheses.
          """
          return doc.xpath(path)[0].text.replace('(','').replace(')','').encode('utf-8')
      def percent(path):
          """ Take xpath to tabular data and clean it up by removing % sign and spaces.
          """
          return doc.xpath(path)[0].text.replace('%','').replace(' ','').encode('utf-8')
      def dollar(path):
          """ Take xpath to tabular data and clean it up by removing $ sign.
          """
          return doc.xpath(path)[0].text.replace('$','').encode('utf-8')
  
      ## EIN (Federal ID)
      charity['ein'] = re.search("\d{2}-\d{7}",doc.xpath("/html/body/div[@id='wrapper']/div[@id='wrapper2']/div[@id='bodywrap']/div[@id='cn_body']/div[@id='cn_body_inner']/div[@id='leftcontent']/div[@id='leftnavcontent']/div[1][@class='rating']/p[1]/a")[0].tail).group().encode('utf-8')
      ## Overall Rating (Out of 70)
      charity['overall_rating'] = rating("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[2]/td[4]")
      ## Efficiency Rating (Out of 40)
      charity['efficiency_rating'] = rating("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[9]/td[4]")
      ## Capactiy Rating (Out of 30)
      charity['capacity_rating'] = rating("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[15]/td[4]")
      ## Overall Rating (Stars)
      try:
          charity['overall_rating_star'] = re.match('\d',doc.xpath("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[1]/td[4]/img")[0].get('alt')).group().encode('utf-8')
      except:
          charity['overall_rating_star'] = 0
      ## Efficiency Rating (Stars)
      try:
          charity['efficiency_rating_star'] = re.match('\d',doc.xpath("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[8]/td[4]/img")[0].get('alt')).group().encode('utf-8')
      except:
          charity['efficiency_rating_star'] = 0
      ## Capacity Rating (Stars)
      try:
          charity['capacity_rating_star'] = re.match('\d',doc.xpath("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[14]/td[4]/img")[0].get('alt')).group().encode('utf-8')
      except:
          charity['capacity_rating_star'] = 0
      ## Program Expenses (as a percentage of TFE)
      charity['program_expense_percent'] = percent("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[4]/td[2]")
      ## Administrative Expenses (as a percentage of TFE)
      charity['admin_expense_percent'] = percent("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[5]/td[2]")
      ## Fundraising Expenses  (as a percentage of TFE)
      charity['fund_expense_percent'] = percent("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[6]/td[2]")
      ## Fundraising Efficiency
      charity['fund_efficiency'] = dollar("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[7]/td[2]")
      ## Primary Revenue Growth
      charity['primary_revenue_growth'] = percent("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[11]/td[2]")
      ## Program Expense Growth
      charity['program_expense_growth'] = percent("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[12]/td[2]")
      ## Working Capital Ratio (Years)
      charity['working_capital_ratio'] = percent("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[1]/div[@class='rating']/table/tr[13]/td[2]")
      ## Primary Revenue
      charity['primary_revenue'] = dollar("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[2]/div[@class='rating']/table/tr[2]/td[2]")
      ## Other Revenue
      charity['other_revenue'] = dollar("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[2]/div[@class='rating']/table/tr[3]/td[2]")
      ## Total Revenue
      charity['total_revenue'] = dollar("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[2]/div[@class='rating']/table/tr[4]/td[2]/strong")
      ## Program Expenses (absolute)
      charity['program_expense'] = dollar("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[2]/div[@class='rating']/table/tr[7]/td[2]")
      ## Administrative Expenses
      charity['admin_expense'] = dollar("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[2]/div[@class='rating']/table/tr[8]/td[2]")
      ## Fundraising Expenses (absolute)
      charity['fund_expense'] = dollar("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[2]/div[@class='rating']/table/tr[9]/td[2]")
      ## Total Functional Expenses
      charity['total_functional_expense'] = dollar("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[2]/div[@class='rating']/table/tr[10]/td[2]/strong")
      ## Payments to Affiliates
      charity['affiliate_payments'] = dollar("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[2]/div[@class='rating']/table/tr[12]/td[2]")
      ## Budget Surplus
      charity['budget_surplus'] = dollar("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[2]/div[@class='rating']/table/tr[13]/td[2]")
      ## Net Assets
      charity['net_assets'] = dollar("//div[@id='summary']/div[2][@class='summarywrap']/div[1][@class='leftcolumn']/div[2]/div[@class='rating']/table/tr[15]/td[2]")
      ## Leadership Compensation
      charity['leader_comp'] = dollar("//div[@id='summary']/div[2][@class='summarywrap']/div[3][@class='bottom']/div[2][@class='leadership']/table/tr[2]/td[3][@class='rightalign']")
      ## Leadership Compensation as % of Expenses
      charity['leader_comp_percent'] = percent("//div[@id='summary']/div[2][@class='summarywrap']/div[3][@class='bottom']/div[2][@class='leadership']/table/tr[2]/td[4][@class='rightalign']")
      ## Website
      charity['website'] = doc.xpath("//div[@id='leftnavcontent']/div[1][@class='rating']/p[2]/a[2]")[0].get('href').encode('utf-8')
      ## E-mail
      charity['e-mail'] = doc.xpath("//div[@id='leftnavcontent']/div[1][@class='rating']/p[2]/a[1]")[0].get('href').replace('mailto:','').encode('utf-8')
#+end_src
** Export CSV File
We want to export using CSV for sharing and using in statistical
software.
#+begin_src python
  with open('output.csv','wb') as f:
      all_fields = results[0].keys()
      ordered_fields = "orgid ein charity_name category city state".split()
      unordered_fields = list(set(all_fields) - set(ordered_fields))
      fn = ordered_fields
      fn.extend(unordered_fields)
      writer=csv.DictWriter(f, fieldnames=fn, extrasaction='ignore')
      headers={}
      for n in fn:
          headers[n]=n
      writer.writerow(headers)
      for charity in results:
          writer.writerow(charity)
#+end_src

